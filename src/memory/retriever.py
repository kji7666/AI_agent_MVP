import uuid
from datetime import datetime
from typing import List
import numpy as np

from langchain_chroma import Chroma
from langchain_core.documents import Document

from src.memory.models import Memory
from src.memory.importance import get_importance_scorer
from src.llm_factory import get_embeddings

class GenerativeRetriever:
    def __init__(self, collection_name: str, decay_factor: float = 0.995):
        self.embeddings = get_embeddings()
        self.vector_store = Chroma(
            collection_name=collection_name,
            embedding_function=self.embeddings,
            client_settings=None, # ä½¿ç”¨ Docker é è¨­
            collection_metadata={"hnsw:space": "cosine"}
        )
        self.importance_scorer = get_importance_scorer()
        self.decay_factor = decay_factor # è«–æ–‡ä¸­çš„éºå¿˜ä¿‚æ•¸

    def add_memory(self, content: str, created_at: datetime = None, type: str = "observation"):
        """
        æ–°å¢žè¨˜æ†¶ï¼šè‡ªå‹•è¨ˆç®— Embedding èˆ‡ Importance
        """
        if created_at is None:
            created_at = datetime.now()

        # 1. å‘¼å« LLM è¨ˆç®—é‡è¦æ€§
        try:
            score = self.importance_scorer.invoke({"memory_content": content})
        except Exception as e:
            print(f"   âš ï¸ Scoring failed, defaulting to 5. Error: {e}")
            score = 5

        # 2. å»ºç«‹è¨˜æ†¶ç‰©ä»¶
        memory = Memory(
            id=str(uuid.uuid4()),
            content=content,
            created_at=created_at,
            last_accessed_at=created_at,
            importance=score,
            type=type
        )

        # 3. å¯«å…¥ Vector DB
        payload = memory.to_chroma_payload()
        self.vector_store.add_documents([
            Document(page_content=payload["page_content"], metadata=payload["metadata"])
        ])
        print(f"   âœ… Saved Memory (Score: {score}): {content}")

    def retrieve(self, query: str, now: datetime = None, k: int = 5, fetch_k: int = 100) -> List[Document]:
        """
        æ··åˆæª¢ç´¢æ ¸å¿ƒé‚è¼¯ï¼š
        1. å…ˆç”¨ Vector Search æŠ“å– Top-100 (Relevance)
        2. è¨ˆç®— Recency èˆ‡ Importance
        3. åŠ æ¬Šç¸½åˆ†æŽ’åºï¼Œå›žå‚³ Top-K
        """
        if now is None:
            now = datetime.now()

        # 1. å‘é‡æª¢ç´¢ (Relevance) - æŠ“å–è¼ƒå¤§ç¯„åœçš„å€™é¸é›†
        # results_with_score å›žå‚³ (Document, distance)
        # Cosine Distance è¶Šå°è¶Šå¥½ï¼Œæˆ‘å€‘è½‰æˆ Similarity (1 - dist)
        candidates = self.vector_store.similarity_search_with_score(query, k=fetch_k)

        if not candidates:
            return []

        # 2. æº–å‚™ç‰¹å¾µå‘é‡
        relevance_scores = []
        recency_scores = []
        importance_scores = []
        
        docs = []

        for doc, distance in candidates:
            docs.append(doc)
            
            # A. Relevance (æ­¸ä¸€åŒ–åˆ° 0-1)
            # Chroma Cosine distance ç¯„åœé€šå¸¸æ˜¯ 0~2ï¼Œé€™è£¡ç°¡å–®è½‰ç‚ºç›¸ä¼¼åº¦ sim
            sim = 1 - distance
            relevance_scores.append(sim)
            
            # B. Importance (æ­£è¦åŒ– 1-10 -> 0-1)
            imp = doc.metadata.get("importance", 1)
            importance_scores.append(imp / 10.0)
            
            # C. Recency (æŒ‡æ•¸è¡°æ¸›)
            # è«–æ–‡å…¬å¼ï¼šdecay_factor ^ (hours_passed)
            last_accessed = datetime.fromtimestamp(doc.metadata.get("last_accessed_at"))
            hours_passed = (now - last_accessed).total_seconds() / 3600
            recency = self.decay_factor ** hours_passed
            recency_scores.append(recency)

        # 3. æ­¸ä¸€åŒ– (Min-Max Scaling)
        # è®“ä¸‰å€‹æŒ‡æ¨™éƒ½åœ¨ 0-1 ä¹‹é–“ï¼Œé€™æ¨£åŠ æ¬Šæ‰æœ‰æ„ç¾©
        def normalize(arr):
            arr = np.array(arr)
            if np.max(arr) == np.min(arr): return arr # é¿å…é™¤ä»¥é›¶
            return (arr - np.min(arr)) / (np.max(arr) - np.min(arr))

        # æ³¨æ„ï¼šé›–ç„¶ä¸Šé¢å·²ç¶“åšäº†ç°¡å–®æ­£è¦åŒ–ï¼Œä½†ç‚ºäº†è®“æ··åˆåˆ†æ•¸åˆ†ä½ˆæ›´å»£ï¼Œ
        # æˆ‘å€‘é€šå¸¸æœƒå°é€™ç¾¤ candidates å†åšä¸€æ¬¡ min-max
        norm_recency = normalize(recency_scores)
        norm_importance = normalize(importance_scores)
        norm_relevance = normalize(relevance_scores)

        # 4. è¨ˆç®—ç¸½åˆ†
        # è«–æ–‡æ¬Šé‡ï¼šalpha=1, beta=1, gamma=1 (å¯èª¿æ•´)
        alpha, beta, gamma = 1.0, 1.0, 1.0
        total_scores = (alpha * norm_recency) + (beta * norm_importance) + (gamma * norm_relevance)

        # 5. æŽ’åºä¸¦å–å‡º Top-K
        # argsort å›žå‚³çš„æ˜¯å¾žå°åˆ°å¤§çš„ indexï¼Œæ‰€ä»¥è¦åè½‰ [::-1]
        top_indices = np.argsort(total_scores)[::-1][:k]
        
        final_results = []
        for idx in top_indices:
            doc = docs[idx]
            # æ›´æ–° last_accessed_at (å› ç‚ºè¢«æƒ³èµ·ä¾†äº†)
            # é€™è£¡æš«æ™‚ä¸å¯«å›ž DB ä»¥å…æ‹–æ…¢é€Ÿåº¦ï¼Œä½†åœ¨å®Œæ•´ç³»çµ±ä¸­æ‡‰è©²è¦æ›´æ–°
            # doc.metadata['last_accessed_at'] = now.timestamp()
            final_results.append(doc)
            
            # Debug è¼¸å‡ºï¼Œè®“ä½ çœ‹åˆ°åˆ†æ•¸æ˜¯æ€Žéº¼ç®—å‡ºä¾†çš„
            print(f"   ðŸ” Rank {len(final_results)}: {doc.page_content}")
            print(f"      Scores -> Recency: {norm_recency[idx]:.2f}, Imp: {norm_importance[idx]:.2f}, Rel: {norm_relevance[idx]:.2f} | Total: {total_scores[idx]:.2f}")

        return final_results