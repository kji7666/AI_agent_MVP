import asyncio
import uuid
import numpy as np
from datetime import datetime
from typing import List, Optional

from langchain_chroma import Chroma
from langchain_core.documents import Document

from src.memory.models import Memory
from src.memory.importance import get_importance_scorer
from src.llm_factory import get_embeddings

class GenerativeRetriever:
    """
    add_memory (æ–°å¢è¨˜æ†¶) ---> å¯«å…¥ DB

    retriever (å›æƒ³) ---> æ›´æ–° last_access_time
                           |
                           +--> push memory.id åˆ° update_queue (å¯é¸)
    _background_flusher (èƒŒæ™¯å®ˆè­·) ---> å–å‡º queue ä¸­çš„ id
                                        |
                                        +--> asyncio.to_thread(_batch_update_access_time)
    _batch_update_access_time (åŒæ­¥) ---> è®€å– metadata -> æ›´æ–° last_accessed_at -> å¯«å› DB
    """
    def __init__(self, collection_name: str, decay_factor: float = 0.995):
        """
        åˆå§‹åŒ–æª¢ç´¢å™¨
        Args:
            collection_name: ChromaDB çš„é›†åˆåç¨±
            decay_factor: è¨˜æ†¶éºå¿˜ä¿‚æ•¸ (è«–æ–‡é è¨­ 0.995)
        """
        # ç”¨ä¾†å°‡æ–‡å­—è½‰æˆå‘é‡ (vector) å„²å­˜æ–¼å‘é‡è³‡æ–™åº«ä¸­ã€‚
        self.embeddings = get_embeddings()
        
        # åˆå§‹åŒ– Chroma Vector Database (å‘é‡æœå°‹ä½¿ç”¨ cosine similarity)
        self.vector_store = Chroma(
            collection_name=collection_name,
            embedding_function=self.embeddings,
            client_settings=None, 
            collection_metadata={"hnsw:space": "cosine"} # ä½¿ç”¨é¤˜å¼¦ç›¸ä¼¼åº¦
        )
        
        # ä½¿ç”¨æœ¬åœ°å°æ¨¡å‹çš„è©•åˆ†å™¨
        self.importance_scorer = get_importance_scorer()
        # è¨˜æ†¶è¡°é€€ä¿‚æ•¸
        self.decay_factor = decay_factor
        
        # å­˜æ”¾å¾…æ›´æ–°è¨˜æ†¶ï¼Œä¸é˜»å¡ä¸»åŸ·è¡Œæµç¨‹
        self.update_queue = asyncio.Queue()
        # å»ºç«‹èƒŒæ™¯å·¥ä½œä»»å‹™ï¼ŒæŒçºŒè™•ç†æ›´æ–°ä½‡åˆ— â†’ å°‡æ›´æ–°å¾Œçš„è¨˜æ†¶æ‰¹é‡å¯«å› Chroma
        self.flusher_task = asyncio.create_task(self._background_flusher())
        print(f"ğŸš€ [Retriever] Initialized with Async Write-back & Local LLM Scoring.")

    async def _background_flusher(self):
        """
        [Background Task] å®šæœŸå°‡ last_accessed_at å¯«å› DB
        é¿å…æª¢ç´¢æ™‚å› ç‚ºå¯«å…¥ DB è€Œè®Šæ…¢ã€‚
        """
        while True:
            try:
                ids_to_update = []
                # å˜—è©¦å°‡ Queue ä¸­çš„æ‰€æœ‰ä»»å‹™å–å‡º
                while not self.update_queue.empty():
                    ids_to_update.append(self.update_queue.get_nowait())
                
                if ids_to_update:
                    # é¿å…åŒä¸€å€‹ ID è¢«å¤šæ¬¡æ›´æ–°, å»é‡è¤‡
                    unique_ids = list(set(ids_to_update))
                    current_time = datetime.now().timestamp()
                    
                    # ChromaDB å¯«å…¥æ˜¯ åŒæ­¥ & é˜»å¡å¼ I/O, è¦ await (éœ€æ”¾å…¥å…¶ä»– thread é¿å…é˜»å¡)
                    await asyncio.to_thread(self._batch_update_access_time, unique_ids, current_time)
                    
                # æ¯ 5 ç§’ loop ä¸€æ¬¡
                await asyncio.sleep(5)
                
            except asyncio.CancelledError:
                print("Flusher task cancelled.")
                break
            except Exception as e:
                print(f"Flusher Error: {e}")
                await asyncio.sleep(5)

    def _batch_update_access_time(self, ids: List[str], timestamp: float):
        """åŒæ­¥çš„ Chroma æ‰¹é‡æ›´æ–°é‚è¼¯ (è¢«ä¸Šé¢çš„ async åŒ…è£)"""
        try:
            # å¿…é ˆå…ˆå–å‡º metadata çš„å…¶ä»–æ¬„ä½ï¼Œå› ç‚º update time æœƒè¦†è“‹æ•´å€‹ metadata
            existing_data = self.vector_store.get(ids=ids)
            
            if existing_data and existing_data['ids']:
                new_metadatas = []
                for meta in existing_data['metadatas']:
                    # æ›´æ–°æ™‚é–“æˆ³
                    meta['last_accessed_at'] = timestamp
                    new_metadatas.append(meta)
                
                # å¯«å› DB
                self.vector_store.update_documents(
                    ids=existing_data['ids'],
                    metadatas=new_metadatas
                )
        except Exception as e:
            print(f"   âš ï¸ Chroma Update Failed: {e}")

    async def add_memory(self, content: str, created_at: datetime = None, type: str = "observation"):
        """
        [Async] æ–°å¢è¨˜æ†¶
        1. å‘¼å«æœ¬åœ° LLM è©•åˆ† (Fast)
        2. å¯«å…¥ Vector DB
        """
        if created_at is None:
            created_at = datetime.now()

        # è¨ˆç®—é‡è¦æ€§
        # ä½¿ç”¨ to_thread é¿å… invoke é˜»å¡ Event Loop
        try:
            score = await asyncio.to_thread(
                self.importance_scorer.invoke, # blocking method
                {"memory_content": content} # method param
            )
        except Exception as e:
            print(f"   âš ï¸ Scoring failed, defaulting to 1. Error: {e}")
            score = 1

        memory = Memory(
            id=str(uuid.uuid4()),
            content=content,
            created_at=created_at,
            last_accessed_at=created_at,
            importance=score,
            type=type
        )
        
        # å¯«å…¥ Vector DB (Async)
        payload = memory.to_chroma_payload()
        await asyncio.to_thread(
            self.vector_store.add_documents,
            [Document(page_content=payload["page_content"], metadata=payload["metadata"])]
        )

        

    async def retrieve(self, query: str, now: datetime = None, k: int = 5, fetch_k: int = 100) -> List[Document]:
        """
        [Async] æ··åˆæª¢ç´¢æ ¸å¿ƒé‚è¼¯
        """
        if now is None:
            now = datetime.now()

        # å‘é‡æª¢ç´¢ (Relevance) - æŠ“å–è¼ƒå¤§ç¯„åœçš„å€™é¸é›†
        # ä½¿ç”¨ to_thread å› ç‚º similarity_search æ˜¯åŒæ­¥ä¸”è€—æ™‚çš„
        candidates = await asyncio.to_thread(
            self.vector_store.similarity_search_with_score,
            query,
            k=fetch_k
        )

        if not candidates:
            return []

        # è¨ˆç®—æ··åˆåˆ†æ•¸
        # è«–æ–‡å…¬å¼: Score = a*Recency + b*Importance + c*Relevance
        docs = [doc for doc, _ in candidates]
        # A. Relevance (Similarity)
        # Chroma å›å‚³çš„æ˜¯ Distance (0~2)ï¼Œè½‰ç‚º Similarity
        relevance_scores = [1.0 - dist for _, dist in candidates]
        # B. Importance (1-10 -> 0-1)
        importance_scores = [doc.metadata.get("importance", 1) / 10.0 for doc in docs]
        # C. Recency (Decay Factor)
        recency_scores = []
        for doc in docs:
            last_accessed_ts = doc.metadata.get("last_accessed_at", now.timestamp())
            last_accessed = datetime.fromtimestamp(last_accessed_ts)
            hours_passed = (now - last_accessed).total_seconds() / 3600
            hours_passed = max(0, hours_passed)
            recency = self.decay_factor ** hours_passed
            recency_scores.append(recency)

        #  (Min-Max Scaling)
        def normalize(arr):
            a = np.array(arr)
            if np.max(a) == np.min(a):
                return a # å¦‚æœæ•¸å€¼éƒ½ä¸€æ¨£ï¼Œå°±ä¸ç¸®æ”¾
            return (a - np.min(a)) / (np.max(a) - np.min(a))

        norm_recency = normalize(recency_scores)
        norm_importance = normalize(importance_scores)
        norm_relevance = normalize(relevance_scores)

        # åŠ æ¬Šç¸½åˆ† (æ¬Šé‡å¯èª¿æ•´)
        alpha, beta, gamma = 1.0, 1.0, 1.0
        total_scores = (alpha * norm_recency) + (beta * norm_importance) + (gamma * norm_relevance)

        # 5. æ’åºä¸¦å–å‡º Top-K
        # argsort æ˜¯å¾å°åˆ°å¤§ï¼Œæ‰€ä»¥ç”¨ [::-1] åè½‰
        top_indices = np.argsort(total_scores)[::-1][:k]
        
        final_results = []
        for idx in top_indices:
            doc = docs[idx]
            final_results.append(doc)
            
            # å°‡æ­¤ ID åŠ å…¥æ›´æ–°ä½‡åˆ—
            # æˆ‘å€‘ä¸ç­‰å¾…å®ƒå¯«å…¥ï¼Œç›´æ¥ç¹¼çºŒ
            doc_id = doc.metadata.get("id")
            if doc_id:
                await self.update_queue.put(doc_id)

        return final_results