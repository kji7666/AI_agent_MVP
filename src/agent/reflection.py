from typing import List
from langchain_core.prompts import ChatPromptTemplate
from src.llm_factory import get_llm
from src.memory.retriever import GenerativeRetriever

class Reflector:
    def __init__(self, retriever: GenerativeRetriever):
        self.retriever = retriever
        self.llm = get_llm(temperature=0.5)

    async def run(self, agent_name: str, last_k: int = 20):
        print(f"ğŸ¤” {agent_name} æ­£åœ¨åæ€æœ€è¿‘ç™¼ç”Ÿçš„äº‹...")
        
        recent_memories = await self.retriever.retrieve(
            query=f"{agent_name} æœ€è¿‘ç™¼ç”Ÿäº†ä»€éº¼äº‹?",
            k=last_k,
            fetch_k=last_k * 2
        )
        
        if not recent_memories:
            print("   æ²’æœ‰è¶³å¤ çš„è¨˜æ†¶å¯ä¾›åæ€ã€‚")
            return

        observations = [m.page_content for m in recent_memories]
        observations_str = "\n".join([f"- {o}" for o in observations])

        prompt = ChatPromptTemplate.from_template("""
        {observations}
        
        åƒ…æ ¹æ“šä»¥ä¸Šè³‡è¨Šï¼Œæˆ‘å€‘å¯ä»¥æ¨æ–·å‡ºé—œæ–¼ {agent_name} çš„å“ª 3 å€‹æœ€é‡è¦çš„é«˜å±¤æ¬¡æ´å¯Ÿ (Insights)ï¼Ÿ
        è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œåˆ—å‡º 3 å€‹ä¸åŒçš„å¥å­ï¼Œæ¯è¡Œä¸€å¥ã€‚ä¸è¦åŒ…å«ç·¨è™Ÿã€‚
        """)
        
        chain = prompt | self.llm
        
        try:
            response = chain.invoke({
                "observations": observations_str, 
                "agent_name": agent_name
            })
            insights = response.content.strip().split('\n') # åˆ—å‡º 3 å€‹ä¸åŒçš„å¥å­ï¼Œæ¯è¡Œä¸€å¥ => \n split
            
            for insight in insights:
                insight = insight.strip()
                if insight and len(insight) > 5: 
                    print(f"   ğŸ’¡ ç”Ÿæˆæ´å¯Ÿ: {insight}")
                    await self.retriever.add_memory(
                        content=insight,
                        type="reflection"
                    )
                    
        except Exception as e:
            print(f"âŒ åæ€å¤±æ•—: {e}")